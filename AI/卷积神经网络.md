# 卷积神经网络

## 前言

本文主要介绍了tensorflow手写数字识别相关的理论，包括卷积，池化，全连接，梯度下降法。

![CNN中卷积层的计算细节](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\v2-aaad3a67aac2b7b1412455eb331b7977_1440w.jpg)

## 手写数字识别相关理论

### 手写数字识别运算方法

![image-20211019172216046](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20211019172216046-16346353376491.png)

识别过程就像图片中那样，经过多次卷积和池化（又叫子采样），最后全连接就运算完成了。

### 卷积

卷积神经网络简介（Convolutional Neural Networks，简称CNN）

卷积神经网络是近年发展起来，并引起广泛重视的一种高效识别方法。20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经

元时发现其独特的网络结构可以有效地降低反馈神经网络的复杂性，继而提出了卷积神经网络（Convolutional Neural Networks-简称CNN）。

现在，CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用。

#### 卷积运算过程

> 计算公式：
>
>  （padding=vaild）height =width = (P-S)/strides +1
>
>  （padding=same）height =width = P/strides



![img](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\format,png.gif)

图2

在图2中。展示了一个3*3的卷积核在5*5的图像上做卷积的过程。每个卷积都是一种特征提取方式，就像一个筛子，将图像中符合条件

（激活值越大越符合条件）的部分筛选出来。

左侧绿色5*5矩阵是输入的图片的灰度值，黄色部分是用来提取特征的卷积核（也可以叫滤波器）。卷积核在图片灰度矩阵上从左到右

，从上到下滑动，每一次滑动两个矩阵对应位置的元素相乘然后求和就可以得到右边矩阵的一个元素。

![image-20211019172308365](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20211019172308365-16346353896802.png)

图3

在图3的左图中，卷积的运算方式是模拟人脑神经网络的运算方式。图3右下角是卷积的数学公式，基本原理是矩阵的对应元素相乘求和，然后加上一个偏振值。

#### 滑动的步长

在上面的计算过程中，卷积核从左到右每次移动一格，也可以一次移动多个。每次移动的格数就是步长。

#### 卷积的边界处理

在上面的计算过程中，计算完成后得到的矩阵只有3*3。因为边界没有了，所以比原来的图片要小。卷积的边界处理方式有两种：

丢掉边界。也就是后面的运算直接按照之前的出的结果来。

复制边界。也就是把源矩阵的最外层数据原封不动的复制过来。

### 池化

池化公式=

![image-20211019172323990](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20211019172323990.png)

图4

池化分为两种：

一种是最大池化，在选中区域中找最大的值作为抽样后的值。另一种是平均值池化，把选中的区域中的平均值作为抽样后的值。

这样做是为了后面全连接的时候减少连接数。

### 全连接

![image-20211019172350055](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20211019172350055-16346354313113.png)

图5

左边的是没有没有进行卷积的全连接，假设图片是1000*1000的，然后用1M的神经元去感知，最后需要10^12个权值作为参数。右边是经过卷积过的，

每个圆点是一个神经元，因此只是用一个卷积核的话，其实只要100*10^6，数量级就大大减少。而且因为提取的就是所需的特征，所以在加快训练

速度的时候对结果并不会产生过大的影响，甚至更为精确。

### 卷积核相关：梯度下降

<img src="E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20211019172409476.png" alt="image-20211019172409476" style="zoom:33%;" />

图6

卷积核是被各种训练集训练出来的，利用梯度下降法可以使得参数到达最优解。

梯度下降的原理：将函数比作一座山，我们站在某个山坡上，往四周看，从哪个方向向下走一小步，能够下降的最快。



## 卷纸神经网络设计思路





###  原则一：最高层的感受野应该不大于图像范围

#### 感受野（receptive filed size）

![img](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\v2-a71f153d40ac8281a39bc0f39d1158ba_720w.jpg)

在[卷积神经网络](https://so.csdn.net/so/search?q=卷积神经网络&spm=1001.2101.3001.7020)中，**感受野的定义**是：卷积神经网络每一层输出的特征图（feature map）上的[像素](https://so.csdn.net/so/search?q=像素&spm=1001.2101.3001.7020)点在原始图像上映射的区域大小。

![image-20220310142553451](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20220310142553451.png)

![img](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\70)

###  C-value的控制（网络学习能力界限）

> 对于每一个卷积层，其学习更复杂表示的能力应该被保证

**C-Value=Real Filter Size/Receptive Filed size**

**C-value>1/6**

![image-20220310145004924](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20220310145004924-16468950065172.png)

![image-20220310144317372](E:\Project\GithubProject\NoteHub\AI\卷积神经网络.assets\image-20220310144317372-16468946453461.png)



## 参考文献

https://cloud.tencent.com/developer/article/1042734?from=14588

https://zhuanlan.zhihu.com/p/78760534

